# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a Retrieval Augmented Generation (RAG) application built on Cloudflare Workers that demonstrates how to combine vector embeddings, database storage, and AI text generation. The application allows users to store notes in a knowledge base and query them using natural language, with AI-generated responses enhanced by relevant context from the stored notes.

## Core Architecture

### Application Stack
- **Framework**: Hono (lightweight web framework)
- **Runtime**: Cloudflare Workers
- **Storage**: Hybrid approach
  - **KV**: Stores full document content (up to 25 MiB per document)
  - **D1**: Stores metadata, document relationships, and note chunks
  - **Vectorize**: Stores vector embeddings with metadata for semantic search
- **Vector Search**: Vectorize for semantic similarity search using `@cf/baai/bge-base-en-v1.5` embeddings
- **AI Models**:
  - Default: Workers AI with `@cf/meta/llama-3.1-8b-instruct`
  - Optional: Anthropic Claude (`claude-3-5-sonnet-latest`) when `ANTHROPIC_API_KEY` is set
- **Orchestration**: Cloudflare Workflows for async document processing with step tracking
- **Text Processing**: LangChain's `RecursiveCharacterTextSplitter` (configurable via `ENABLE_TEXT_SPLITTING`)

### Key Components

**src/index.ts** - Main application entry point:
- Hono app with REST API endpoints for documents, queries, and chat
- `RAGWorkflow` class implementing Cloudflare Workflows for async document processing
- Workflow pipeline orchestrates: document storage → metadata creation → text splitting → embedding generation → vector storage
- Request handlers for document management, RAG queries, and chat conversations

**src/utils/logger.ts** - Structured logging utility:
- Provides context-aware logging with request tracking
- Timer tracking for performance monitoring
- Error logging with stack traces

**src/utils/document-store.ts** - Document management abstraction:
- Encapsulates KV, D1, and Vectorize interactions
- Handles document lifecycle: creation, retrieval, deletion
- Manages metadata and chunk relationships

**src/types/index.ts** - TypeScript type definitions:
- `Env`: Bindings for AI, DATABASE, VECTOR_INDEX, RAG_WORKFLOW, DOCUMENTS
- `NoteRecord`, `VectorMetadata`: Database and vector store schemas
- `CreateDocumentInput`, `Document`: Document API contracts

**Workflow Pattern**: When documents are uploaded via `POST /notes`, the RAG workflow:
1. Stores full content in KV with generated UUID
2. Creates metadata record in D1 documents table
3. Optionally splits text into chunks using `RecursiveCharacterTextSplitter`
4. Creates note records in D1 for each chunk (linking to source document)
5. Generates embeddings for each chunk
6. Stores vectors in Vectorize with metadata (document_id, chunk_index)

**RAG Query Flow** (GET / endpoint):
1. Convert user question to embeddings using `@cf/baai/bge-base-en-v1.5`
2. Query Vectorize for top 3 similar vectors
3. Retrieve matching note chunks from D1
4. Load full document content from KV using document_id from vector metadata
5. Construct context from chunks and source documents
6. Generate AI response using context via Workers AI or Anthropic Claude

### Database Schema

The application uses two main D1 tables (created via migrations):

```sql
-- Stores document metadata and full document references
CREATE TABLE documents (
  id TEXT PRIMARY KEY,
  title TEXT,
  author TEXT,
  content_type TEXT,
  metadata TEXT,  -- JSON string with tags, description, and custom metadata
  created_at DATETIME,
  updated_at DATETIME
);

-- Stores text chunks with reference to source document
CREATE TABLE notes (
  id TEXT PRIMARY KEY,
  text TEXT NOT NULL,
  document_id TEXT,
  chunk_index INTEGER,
  FOREIGN KEY (document_id) REFERENCES documents(id)
);
```

**Important**: The `notes.id` field is auto-generated by D1 and used as the vector ID in Vectorize for matching database records to vector search results. Vectors store metadata including `document_id` and `chunk_index` to enable full document retrieval during RAG queries.

## Development Commands

### Local Development
```bash
npm install                    # Install dependencies
npm run start                 # Start local dev server (wrangler dev)
npm run test                  # Run all tests (Vitest)
npm run test:watch           # Run tests in watch mode
npm run test:coverage        # Run tests with coverage report
```

### Type Checking & Building
```bash
# Type check the project (no build output due to noEmit: true in tsconfig)
npx tsc --noEmit

# The actual build happens during deployment
```

### Deployment
```bash
npm run deploy                # Deploy to Cloudflare Workers (wrangler deploy)
```

### Database Operations
```bash
# Create new D1 database
wrangler d1 create DATABASE

# Apply migrations locally
wrangler d1 migrations apply DATABASE

# Apply migrations to production
wrangler d1 migrations apply DATABASE --remote

# Query database directly (local)
wrangler d1 execute DATABASE --command="SELECT * FROM notes"

# Query database directly (remote)
wrangler d1 execute DATABASE --remote --command="SELECT * FROM notes"
```

### Vector Index Operations
```bash
# Create vector index (must match embedding model dimensions)
wrangler vectorize:index create VECTOR_INDEX --preset "@cf/baai/bge-base-en-v1.5"
```

### Configuration

**wrangler.jsonc** (JSON with comments format) must include bindings for:
- `AI` - Workers AI binding
- `DATABASE` - D1 database binding with `database_id`
- `VECTOR_INDEX` - Vectorize index binding with `index_name`
- `RAG_WORKFLOW` - Workflow binding with `class_name = "RAGWorkflow"`

### Secrets Management
```bash
# Enable Anthropic Claude (optional)
wrangler secret put ANTHROPIC_API_KEY
```

## API Endpoints

### Query Endpoints
- `GET /` - Query AI with `?text=<question>` parameter, returns text response
- `GET /ui` - Web UI for asking questions with AI responses and source attribution
- `GET /chat` - Chat interface with conversation history support

### Chat API (Conversation Memory & RAG)
- `POST /chat/conversations` - Create new conversation session, returns conversation ID
- `GET /chat/conversations/:id` - Get full conversation history with message ordering
- `POST /chat/conversations/:id/messages` - Send message, triggers RAG pipeline, returns response with source attribution

**Chat Features**:
- Conversation persistence across sessions
- Full message history with timestamps
- Automatic vector embedding for semantic search
- System prompt enforces document-only responses
- Source attribution: Retrieved documents shown with IDs and content preview

### Document Management
- `POST /notes` - Add document to knowledge base (JSON body with `text`, optional `title`, `contentType`, `metadata`)
- `GET /documents` - JSON endpoint listing all documents with metadata
- `GET /documents/:id` - JSON endpoint retrieving specific document with full content and chunks
- `GET /documents/ui` - Web UI to browse all stored documents

### Legacy Notes Endpoints
- `GET /notes` - HTML view of all note chunks
- `GET /notes.json` - JSON array of all note chunks
- `DELETE /notes/:id` - Delete note chunk from both D1 and Vectorize
- `GET /write` - HTML form interface for adding notes (legacy)

## Configuration Options

**ENABLE_TEXT_SPLITTING** (wrangler.jsonc `vars` section):
- `true` (default): Uses `RecursiveCharacterTextSplitter` to break large texts into chunks
- `false`: Stores entire text as single note/embedding

Text splitting is recommended for long documents to improve retrieval accuracy. Customize chunk size/overlap in src/index.ts:150-151.

## Model Switching

The application automatically uses Anthropic Claude if `ANTHROPIC_API_KEY` secret is set, otherwise falls back to Workers AI Llama model. The model used is returned in the `x-model-used` response header.

## TypeScript

TypeScript configuration is in tsconfig.json. The project uses:
- `@cloudflare/workers-types` (v4.20251111.0) for Workers API types
- Target: ES2022 with ESNext modules
- Module resolution: bundler (optimized for Cloudflare Workers)
- Strict type checking enabled

**Important**: When using Workers AI embeddings API, always wrap text in an array and add type assertion:
```typescript
const embeddings = await env.AI.run('@cf/baai/bge-base-en-v1.5', {
  text: [question]
}) as { data: number[][] }
```

## Recent Updates (2025)

This repository has been modernized with the following updates:

### Dependencies
- **Wrangler**: Upgraded to v4.46.0 (from v3.22.4)
  - Now uses esbuild v0.24 (security fix)
  - Local mode by default for all commands
- **Anthropic SDK**: v0.68.0 (from v0.32.1)
- **LangChain Text Splitters**: v1.0.0 (from v0.1.0)
- **Hono**: v4.10.4 (from v4.5.6)
- **TypeScript**: v5.9.3 (from v5.6.3)

### Configuration
- Converted from wrangler.toml to wrangler.jsonc (JSON with comments format)
- Compatibility date updated to 2025-01-10
- TypeScript target updated to ES2022
- Module resolution set to "bundler" for better Workers compatibility

### Known Limitations
- Vectorize bindings do not support local development; use remote bindings for testing vector operations
- AI bindings always access remote resources and may incur usage charges even in local dev

## Development Guidelines

### Before Committing
- Run type checking: `npx tsc --noEmit`
- Run tests: `npm test`
- Verify all tests pass before committing

### Testing Strategy
- **Unit Tests**: Test utilities like Logger and DocumentStore in isolation
- **Integration Tests**: Test API handlers with mocked Cloudflare bindings
- Mocking approach documented in `tests/README.md` for KV, D1, Vectorize, and AI bindings
- Test coverage goals: 90%+ for utilities, 80%+ for business logic

### Workflow Development
When modifying `RAGWorkflow`:
1. Each step in the workflow (text splitting, embedding generation, etc.) should be independently tracked
2. Workflow steps enable reliability and observability of async document processing
3. Errors in workflow steps are logged with context for debugging

### Document Ingestion
When adding features that process documents:
- Consider text splitting behavior (`ENABLE_TEXT_SPLITTING` config)
- Metadata is stored in D1, full content in KV for optimal performance
- Vector metadata includes document_id for traceability back to source documents

### Chat Feature Maintenance
The chat feature (`/chat` path) implements RAG with conversation memory:

**System Prompt Constraints**:
- 5-rule prompt enforces document-only responses
- "Retrieved Documents" section contains top K=3 matching notes
- Fallback message for queries with no relevant documents
- Citation format `[ID: <note-id>]` enables source tracking

**Conversation Management**:
- Each chat session gets unique conversation ID (UUID)
- Messages stored in D1 with conversation_id foreign key
- Source attribution: matched notes serialized as JSON in message record
- Timestamps enable sorting and conversation analytics

**When Modifying Chat**:
1. Never weaken RAG constraints in system prompt
2. Top K value (3) balances context relevance vs token usage
3. Retrieved notes passed to context include note ID and full text
4. Sources returned to client for UI rendering
5. Test with documents and verify citation format in responses
